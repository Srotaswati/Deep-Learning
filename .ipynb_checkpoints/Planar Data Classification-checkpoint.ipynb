{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planar Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a deep L-layer NN for supervised learning to plot decision boundaries for scatter plot data by the following:\n",
    "* Gradient Descent\n",
    "* Gradient Descent with Momentum\n",
    "* Adam Optimization\n",
    "\n",
    "with batch/ mini-batch training sets\n",
    "\n",
    "Hyperparameters:\n",
    "* Learning rate\n",
    "* Mini-batch size\n",
    "* Momentum $\\beta_1$\n",
    "* RMS Momentum $\\beta_2$\n",
    "* Floating point parameter $\\epsilon$\n",
    "* L2 Regularization parameter $\\lambda$\n",
    "* Dropout probability\n",
    "\n",
    "The data is loaded from sklearn.datasets. Utility functions implementing piecemeal functions for loading the dataset and running GD including parameter initialization, forward propagation, backward propagation, cost computation and prediction is stored on mltuils.py imported at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>1. Import Packages<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from mlutils import * # utility functions\n",
    "from getdata import * # load dataset functions\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>2. Explore the Dataset and Reshape<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dataset() missing 2 required positional arguments: 'train_file' and 'test_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-caf555d155d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mflower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoisy_circles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoisy_moons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaussian_quantiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m datasets = {\"flower\": flower,\n\u001b[0;32m      5\u001b[0m             \u001b[1;34m\"noisy_circles\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnoisy_circles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: load_dataset() missing 2 required positional arguments: 'train_file' and 'test_file'"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "flower, noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_data()\n",
    "\n",
    "datasets = {\"flower\": flower,\n",
    "            \"noisy_circles\": noisy_circles,\n",
    "            \"noisy_moons\": noisy_moons,\n",
    "            \"blobs\": blobs,\n",
    "            \"gaussian_quantiles\": gaussian_quantiles}\n",
    "\n",
    "dataset = \"noisy_moons\" # choose dataset\n",
    "\n",
    "train_x, train_y = datasets[dataset]\n",
    "train_x, train_y = train_x.T, train_y.reshape(1, train_y.shape[0])\n",
    "\n",
    "# make blobs binary\n",
    "if dataset == \"blobs\":\n",
    "    train_y = train_y%2\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(train_x[0, :], train_x[1, :], c=train_y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>3. Train L_layer Gradient Descent model<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, layers_dims, optimizer=\"gd\", num_epochs=3000, mini_batch_size=64, beta1=0.9, beta2=0.999, epsilon=1e-8, print_cost=False, learning_rate=0.0007, lambd=0, keep_prob=1):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = [] \n",
    "    t = 0\n",
    "    seed = 10\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Parameters initialization. \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. Increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            # Forward propagation:       \n",
    "            AL, caches = forward_prop(minibatch_X, parameters, keep_prob)\n",
    "        \n",
    "            # Compute cost.\n",
    "            cost +=  compute_cost(AL, minibatch_Y, parameters, lambd)\n",
    "    \n",
    "            # Backward propagation.\n",
    "            grads = back_prop(AL, minibatch_Y, caches, lambd, keep_prob)\n",
    " \n",
    "            # Update parameters.\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n",
    "            ### END CODE HERE ###\n",
    "                \n",
    "            # Print the cost every 100 training example\n",
    "        cost_avg = cost / m\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost_avg))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Model with \" + optimizer + \" optimization\")\n",
    "    axes = plt.gca()\n",
    "    plot_decision_boundary(lambda x: predict_dec(parameters, x.T, keep_prob), X, Y)\n",
    "    \n",
    "    # print accuracy\n",
    "    pred_train = predict(X, Y, parameters, keep_prob)\n",
    "    \n",
    "    result = {\"costs\" : costs,\n",
    "         \"Y_prediction_train\" : pred_train, \n",
    "         \"keep probability\" : keep_prob, \n",
    "         \"regularization parameter\" : lambd,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_epochs\": num_epochs,\n",
    "         \"parameters\" : parameters,\n",
    "         \"gradients\" : grads}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='darkblue'>4. Use Model to Predict<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [train_x.shape[0], 5, 2, 1]\n",
    "model = {}\n",
    "with timer() as elapsed:\n",
    "    model = neural_network(train_x, train_y, layers_dims, mini_batch_size=train_x.shape[1], num_epochs = 10000, lambd=0.7, print_cost = False)\n",
    "    plt.show()\n",
    "print(\" Batch Gradient Descent with regularization took {:d} min {:.2f} sec\".format(int(elapsed()//60), elapsed()%60))\n",
    "print ('\\n' + \"---------------------------------------------------------------------\" + '\\n')\n",
    "\n",
    "with timer() as elapsed:\n",
    "    model = neural_network(train_x, train_y, layers_dims, num_epochs = 10000, print_cost = False)\n",
    "    plt.show()\n",
    "print(\" Mini-Batch Gradient Descent took {:d} min {:.2f} sec\".format(int(elapsed()//60), elapsed()%60))\n",
    "print ('\\n' + \"----------------------------------------------------------------------\" + '\\n')\n",
    "\n",
    "with timer() as elapsed:\n",
    "    model = neural_network(train_x, train_y, layers_dims, optimizer=\"adam\", num_epochs = 10000, print_cost = False)\n",
    "    plt.show()\n",
    "print(\" Mini-batch Adam took {:d} min {:.2f} sec\".format(int(elapsed()//60), elapsed()%60))\n",
    "print ('\\n' + \"----------------------------------------------------------------------\" + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>5. Test model hyperparameters<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.02, 0.009, 0.007]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    with nostdout():\n",
    "        models[str(i)] = neural_network(train_x, train_y, layers_dims, num_epochs=1500, learning_rate=i, print_cost=False)\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
